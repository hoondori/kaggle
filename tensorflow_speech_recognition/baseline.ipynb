{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import os \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "DATA_DIR = '/Users/seonghoonjung/.kaggle/data/tensorflow-speech-recognition'\n",
    "TRAIN_AUDIO_DIR = DATA_DIR + '/train/audio/'\n",
    "TEST_AUDIO_DIR = DATA_DIR + '/test/audio/'\n",
    "\n",
    "SEED=2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터를 화자 기반으로 훈련:검증 데이터로 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "def random_shuffle(lst):\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(lst)\n",
    "    return lst\n",
    "\n",
    "# 결과 저장 폴더\n",
    "if not os.path.exists('input'):\n",
    "    os.mkdir('input')\n",
    "    \n",
    "# 훈련 데이터 전체를 (레이블,화자,파일명) 형태로 메타 파일에 저장한다. \n",
    "trn_all_file = open('input/trn_all.txt', 'w')\n",
    "trn_all = []\n",
    "\n",
    "files = glob(TRAIN_AUDIO_DIR+'/*/*.wav')\n",
    "for f in files:\n",
    "    # 배경 노이즈는 제외 \n",
    "    if '_background_noise_' in f:\n",
    "        continue\n",
    "    \n",
    "    # 레이블과 화자 정보를 파일명에서 추출\n",
    "    label = f.split('/')[-2]\n",
    "    speaker = f.split('/')[-1].split('_')[0]\n",
    "    \n",
    "    # 지정된 레이블이 아니면 unknown 처리\n",
    "    if label not in labels:\n",
    "        label = 'unknown'\n",
    "        if random.random() < 0.2:  # 20% 만 사용 \n",
    "            trn_all.append((label,speaker,f))\n",
    "            trn_all_file.write('{},{},{}\\n'.format(label, speaker, f))\n",
    "    else:\n",
    "        trn_all.append((label,speaker,f))\n",
    "        trn_all_file.write('{},{},{}\\n'.format(label, speaker, f))        \n",
    "trn_all_file.close()\n",
    "\n",
    "# 화자 기준으로 9:1로 학습:검증을 분리한다. \n",
    "unique_speakers = list(set([speaker for (label, speaker, path) in trn_all]))\n",
    "random_shuffle(unique_speakers)\n",
    "cutoff = int(len(unique_speakers) * 0.9)\n",
    "speaker_val = unique_speakers[cutoff:]\n",
    "\n",
    "# 교차 검증용 파일을 생성한다. \n",
    "trn_file = open('input/trn.txt', 'w')\n",
    "val_file = open('input/val.txt', 'w')\n",
    "for (label, speaker, path) in trn_all:\n",
    "    if speaker not in speaker_val:\n",
    "        trn_file.write('{},{},{}\\n'.format(label, speaker, f))\n",
    "    else:\n",
    "        val_file.write('{},{},{}\\n'.format(label, speaker, f))    \n",
    "trn_file.close()\n",
    "val_file.close()\n",
    "\n",
    "# 테스트 파일에 대해서도 메타 파일 생성 (레이블과 화자 정보 없음)\n",
    "tst_all_file = open('input/tst.txt', 'w')\n",
    "files = glob(TEST_AUDIO_DIR+'/*.wav')\n",
    "for f in files:\n",
    "    tst_all_file.write(',,{}\\n'.format(f))    \n",
    "tst_all_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 루틴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# 샘플링 비율\n",
    "SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, mode, label_to_int, wav_list, label_list=None):\n",
    "        self.mode = mode\n",
    "        self.label_to_int = label_to_int # 문자 -> 숫자 레이블 전환 \n",
    "        self.wav_list = wav_list\n",
    "        self.label_list = label_list  # 문자로 된 label\n",
    "        self.sr = SR\n",
    "        self.n_silence = int(len(wav_list) * 0.1)  # 묵음은 전체 데이터의 10%\n",
    "        \n",
    "        # 배경 데이터를 미리 읽어온다. \n",
    "        # 데이터 augmentation에 사용\n",
    "        self.bg_noises = [librosa.load(f, sr=self.sr)[0] for f in glob(TRAIN_AUDIO_DIR+'/_background_noise_/*.wav')]\n",
    "        \n",
    "    # 목록에서 idx번째 음성 데이터를 전달(1초만)\n",
    "    def get_one_word_wav(self, idx):\n",
    "        wav = librosa.load(self.wav_list[idx], sr=self.sr)[0]\n",
    "        if len(wav) < self.sr: # 1초보다 모자르면 뒤에 0으로 패딩\n",
    "            wav = np.pad(wav, (0, self.sr - len(wav)), 'constant')\n",
    "        return wav[:self.sr]\n",
    "    \n",
    "        \n",
    "    # 배경 노이즈 목록에서 랜덤하게 1초만 전달    \n",
    "    def get_one_noise(self):\n",
    "        # 노이즈 랜덤 선택\n",
    "        noise = self.bg_noises[random.randint(0,len(self.bg_noises)-1)]\n",
    "        \n",
    "        # 시작 위치 랜덤 선택\n",
    "        start_idx = random.randint(0, len(noise)-1-self.sr)\n",
    "        \n",
    "        return noise[start_idx:(start_idx+self.sr)]\n",
    "    \n",
    "    # N개의 배경 노이즈를 합성하여 전달\n",
    "    def get_mix_noises(self, num_noise=1, max_ratio=0.1):\n",
    "        result = np.zeros(self.sr)\n",
    "        for _ in range(num_noise):\n",
    "            result += random.random() * max_ratio * self.get_one_noise()\n",
    "        return result / num_noise if num_noise >0 else result\n",
    "    \n",
    "    def get_silent_wav(self, num_noise=1, max_ratio=0.5):\n",
    "        return self.get_mix_noises(num_noise=num_noise, max_ratio=max_ratio)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode == 'test':\n",
    "            return len(self.wav_list)\n",
    "        else:\n",
    "            return len(self.wav_list) + self.n_silence # 교차 검증일 때는 silence를 추가한 만큼이 데이터 크기\n",
    "        \n",
    "     \n",
    "    # i번째 음성 데이터의 스펙트럼을 전달 \n",
    "    # 목록 이상을 요청하는 경우는 배경 노이즈를 전달\n",
    "    def __getitem__(self,idx):\n",
    "        if idx < len(self.wav_list):\n",
    "            spec_numpy = self.preprocess_mel(self.get_one_word_wav(idx))\n",
    "            spec_tensor = torch.from_numpy(spec_numpy).float()\n",
    "            spec_tensor = spec_tensor.unsqueeze(0) # to 2D\n",
    "            \n",
    "            # feature와 파일 경로, label 전달\n",
    "            if self.mode == 'test':\n",
    "                return { 'spec': spec_tensor, 'id': self.wav_list[idx]}\n",
    "            else:\n",
    "                label = self.label_to_int.get(self.label_list[idx], len(self.label_to_int))\n",
    "                return { 'spec': spec_tensor, 'id': self.wav_list[idx], 'label': label}\n",
    "                \n",
    "        else:\n",
    "             # 노이즈 전달\n",
    "            spec_numpy = self.preprocess_mel(\n",
    "                self.get_silent_wav(\n",
    "                    num_noise=random.choice([0,1,2,3]),\n",
    "                    max_ratio=random.choice([x/10 for x in range(20)]))\n",
    "            )\n",
    "            spec_tensor = torch.from_numpy(spec_numpy).float()\n",
    "            spec_tensor = spec_tensor.unsqueeze(0) # to 2D    \n",
    "            return { 'spec': spec_tensor, 'id': 'silence', 'label': len(self.label_to_int) + 1}  # 라벨 index는 번외 +1 \n",
    "            \n",
    "    \n",
    "    def preprocess_mel(self, data, n_mels=40):\n",
    "        spectrogram = librosa.feature.melspectrogram(data, sr=SR, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
    "        spectrogram = librosa.power_to_db(spectrogram)\n",
    "        spectrogram = spectrogram.astype(np.float32) \n",
    "        return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MaxPool2d\n",
    "\n",
    "# ResNet 구현 \n",
    "class ResModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResModel, self).__init__()\n",
    "        \n",
    "        n_labels = 12\n",
    "        n_maps = 128 \n",
    "        \n",
    "        # 총 9계층\n",
    "        self.n_layers = n_layers = 9\n",
    "        \n",
    "        # 모듈 정의\n",
    "        self.conv0 = torch.nn.Conv2d(1, n_maps, (3,3), padding=(1,1), bias=False)\n",
    "        self.pool = MaxPool2d(2, return_indices=True)\n",
    "        self.convs = torch.nn.ModuleList(\n",
    "            [torch.nn.Conv2d(n_maps, n_maps, (3,3), padding=(1,1), dilation=1, bias=False) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "        # 조합\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # BN - 9 convs\n",
    "            self.add_module(\"bn{}\".format(i+1), torch.nn.BatchNorm2d(n_maps, affine=False))\n",
    "            self.add_module(\"conv{}\".format(i+1), conv)\n",
    "\n",
    "        # 최종 계층은 선형 모듈\n",
    "        self.output = torch.nn.Linear(n_maps, n_labels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for i in range(self.n_layers+1):\n",
    "            y = F.relu(getattr(self, \"conv{}\".format(i))(x))\n",
    "            \n",
    "            # residual 구현 - 2계층마다 residual\n",
    "            if i == 0:\n",
    "                old_x = y\n",
    "            if i > 0 and i%2 == 0:\n",
    "                x = y + old_x\n",
    "                old_x = x\n",
    "            else:\n",
    "                x = y\n",
    "            \n",
    "            # BatchNorm\n",
    "            if i > 0:\n",
    "                x = getattr(self, \"bn{}\".format(i))(x)\n",
    "            \n",
    "            # pooling\n",
    "            pooling = False\n",
    "            if pooling:\n",
    "                x_pool, pool_indices = self.pool(x)\n",
    "                x = self.unpool(x_pool. pool_indices, output_size=x.size())\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = torch.mean(x, 2)\n",
    "        \n",
    "        # 최종 선형 계층을 통과한 결과값 반환\n",
    "        return self.output(x)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from time import time\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_directory(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def get_time(now, start):\n",
    "    time_in_min = int((now - start) / 60)\n",
    "    return time_in_min\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "mGPU = False\n",
    "epochs = 1 #20\n",
    "mode = 'cv'\n",
    "model_name = 'model/model_resnet.pth'\n",
    "\n",
    "# 모델 정의\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model = ResModel\n",
    "speechModel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "#speechModel = speechModel.cuda()\n",
    "\n",
    "# Dataset 정의\n",
    "labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "label_to_int = dict(zip(labels, range(len(labels))))\n",
    "int_to_label = dict(zip(range(len(labels)), labels))\n",
    "int_to_label.update({len(labels): 'unknown', len(labels)+1:'silence'})\n",
    "\n",
    "trn = 'input/trn.txt' if mode == 'cv' else 'input/trn_all.txt'\n",
    "tst = 'input/val.txt' if mode == 'cv' else 'input/tst.txt'\n",
    "\n",
    "# Train DataSet 활성화 \n",
    "trn_size = BATCH_SIZE # * 10\n",
    "trn = [line.strip()for line in open(trn, 'r').readlines()]\n",
    "wav_list = [line.split(',')[-1] for line in trn][:trn_size]  \n",
    "label_list = [line.split(',')[0] for line in trn][:trn_size]\n",
    "trainDataset = SpeechDataset(mode='train', label_to_int=label_to_int, wav_list=wav_list, label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:17<00:17, 17.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:19<00:00,  9.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: tensor(0.) 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seonghoonjung/anaconda3/envs/kaggle/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy: 0.0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for e in range(epochs):\n",
    "    print('training epoch ', e)\n",
    "    \n",
    "    # lr decay\n",
    "    learning_rate = 0.01 if e < 10 else 0.001\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, speechModel.parameters()), lr=learning_rate, momentum=0.9, weight_decay=0.00001)\n",
    "    \n",
    "    # train mode\n",
    "    speechModel.train()\n",
    "    \n",
    "    total_correct = 0\n",
    "    num_labels = 0\n",
    "    \n",
    "    trainLoader = DataLoader(trainDataset, BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 학습을 수행한다. \n",
    "    for batch_idx, batch_data in enumerate(tqdm(trainLoader)):\n",
    "        spec = batch_data['spec']\n",
    "        label = batch_data['label']\n",
    "        #spec, label = Variable(spec).cuda(), Variable(label).cuda()\n",
    "        \n",
    "        # 예측값 계산 \n",
    "        y_pred = speechModel(spec)\n",
    "        _, pred_labels = torch.max(y_pred.data, 1)\n",
    "        correct = (pred_labels == label.data).sum()\n",
    "        \n",
    "        total_correct += correct\n",
    "        num_labels += len(label)\n",
    "        \n",
    "        # loss 계산 \n",
    "        loss = loss_fn(y_pred, label) \n",
    "        \n",
    "        # 역전파 \n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        \n",
    "        # 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "    # 정확도 계산\n",
    "    print('training accuracy:', 100. * total_correct/num_labels, get_time(time(), start_time))\n",
    "        \n",
    "    # 교차 검증 모드의 경우, 검증 데이터에 대한 정확률을 기록한다\n",
    "    if mode == 'cv':\n",
    "        # 현재 학습 중인 모델을 임시로 저장한다\n",
    "        torch.save(speechModel.state_dict(), '{}_cv'.format(model_name))\n",
    "        \n",
    "        # 검증 데이터를 불러온다\n",
    "        val_size = 10\n",
    "        softmax = Softmax()\n",
    "        tst_list = [line.strip() for line in open(tst, 'r').readlines()]\n",
    "        wav_list = [line.split(',')[-1] for line in tst_list][:val_size]\n",
    "        label_list = [line.split(',')[0] for line in tst_list][:val_size]\n",
    "        cvdataset = SpeechDataset(mode='test', label_to_int=label_to_int, wav_list=wav_list)\n",
    "        cvloader = DataLoader(cvdataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # 모델을 불러와 .eval() 함수로 검증 준비를 한다\n",
    "        speechmodel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "        speechmodel.load_state_dict(torch.load('{}_cv'.format(model_name)))\n",
    "        #speechmodel = speechmodel.cuda()\n",
    "        speechmodel.eval()\n",
    "\n",
    "        # 검증 데이터를 batch_size만큼씩 받아오며 예측값을 저장한다\n",
    "        fnames, preds = [], []\n",
    "        for batch_idx, batch_data in enumerate(tqdm(cvloader)):\n",
    "            spec = Variable(batch_data['spec'])\n",
    "            fname = batch_data['id']\n",
    "            y_pred = softmax(speechmodel(spec))\n",
    "            preds.append(y_pred.data.cpu().numpy())\n",
    "            fnames += fname\n",
    "\n",
    "        preds = np.vstack(preds)\n",
    "        preds = [int_to_label[x] for x in np.argmax(preds, 1)]\n",
    "        fnames = [fname.split('/')[-2] for fname in fnames]\n",
    "        num_correct = 0\n",
    "        for true, pred in zip(fnames, preds):\n",
    "            if true == pred:\n",
    "                num_correct += 1\n",
    "\n",
    "        # 검증 데이터의 정확률을 기록한다\n",
    "        print(\"cv accuracy:\", 100. * num_correct / len(preds), get_time(time(), start_time))\n",
    "        \n",
    "# 학습이 완료된 모델을 저장한다\n",
    "create_directory(\"model\")\n",
    "torch.save(speechmodel.state_dict(), model_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터에 대해 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seonghoonjung/anaconda3/envs/kaggle/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "tst = 'input/tst.txt'\n",
    "\n",
    "# 테스트 데이터에 대한 예측값을 파일에 저장한다\n",
    "print(\"doing prediction...\")\n",
    "softmax = Softmax()\n",
    "\n",
    "# 테스트 데이터를 불러온다\n",
    "tst_size = 10\n",
    "tst = [line.strip() for line in open(tst, 'r').readlines()]\n",
    "wav_list = [line.split(',')[-1] for line in tst][:tst_size]\n",
    "testdataset = SpeechDataset(mode='test', label_to_int=label_to_int, wav_list=wav_list)\n",
    "testloader = DataLoader(testdataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델을 불러온다\n",
    "speechmodel = torch.nn.DataParallel(model()) if mGPU else model()\n",
    "speechmodel.load_state_dict(torch.load(model_name))\n",
    "#speechmodel = speechmodel.cuda()\n",
    "speechmodel.eval()\n",
    "    \n",
    "test_fnames, test_labels = [], []\n",
    "pred_scores = []\n",
    "\n",
    "# 테스트 데이터에 대한 예측값을 계산한다\n",
    "for batch_idx, batch_data in enumerate(tqdm(testloader)):\n",
    "    #spec = Variable(batch_data['spec'].cuda())\n",
    "    spec = Variable(batch_data['spec'])\n",
    "    fname = batch_data['id']\n",
    "    y_pred = softmax(speechmodel(spec))\n",
    "    pred_scores.append(y_pred.data.cpu().numpy())\n",
    "    test_fnames += fname\n",
    "\n",
    "# 가장 높은 확률값을 가진 예측값을 label 형태로 저장한다\n",
    "final_pred = np.vstack(pred_scores)\n",
    "final_labels = [int_to_label[x] for x in np.argmax(final_pred, 1)]\n",
    "test_fnames = [x.split(\"/\")[-1] for x in test_fnames]\n",
    "\n",
    "# 테스트 파일 명과 예측값을 sub 폴더 아래 저장한다. 캐글에 직접 업로드 할 수 있는 파일 포맷이다.\n",
    "create_directory(\"sub\")\n",
    "pd.DataFrame({'fname': test_fnames, 'label': final_labels}).to_csv(\"sub/{}.csv\".format(model_name.split('/')[-1]), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kaggle)",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
